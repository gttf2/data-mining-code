import os
import json
import re

# ================= é…ç½®åŒºåŸŸ =================
SOURCE_DIRECTORY = './data/'          # ä½ çš„ .md æ–‡ä»¶ç›®å½•
OUTPUT_JSON_PATH = './data/rag_corpus_final.json'
CHUNK_SIZE = 512                      # æ¯ä¸ªå—çš„å­—ç¬¦ä¸Šé™
CHUNK_OVERLAP = 50                    # é‡å å­—ç¬¦æ•°
# ===========================================

def clean_markdown_content(text):
    """
    V3 å¼ºåŠ›æ¸…æ´—å‡½æ•°ï¼šæˆªæ–­å¹¿å‘Šã€ç§»é™¤å¹²æ‰°ç¬¦
    """
    if not text:
        return ""

    # 1. æ ¸æ­¦å™¨çº§å»å™ªï¼šä»â€œå…¬ç›Šé—®è¯Šâ€å¤„æˆªæ–­ï¼Œä¸¢å¼ƒåé¢æ‰€æœ‰å†…å®¹ï¼ˆåŒ…æ‹¬èœå•ã€ç‚¹èµç­‰ï¼‰
    if "å…¬ç›Šé—®è¯Š" in text:
        text = text.split("å…¬ç›Šé—®è¯Š")[0]
    
    # 2. ç§»é™¤å›¾ç‰‡æ ‡ç­¾ ![...](...) 
    text = re.sub(r'!\[.*?\]\(.*?\)', '', text)
    
    # 3. ç§»é™¤é“¾æ¥ [text](url) -> åªä¿ç•™ text
    text = re.sub(r'\[(.*?)\]\(.*?\)', r'\1', text)
    
    # 4. ç§»é™¤å¤´éƒ¨å¼•å¯¼è¯­
    text = re.sub(r'ç‚¹å‡»ä¸Šæ–¹.*?å…³æ³¨æ›´å¤šç²¾å½©', '', text)
    text = re.sub(r'ç‚¹å‡»è“å­—\s*å…³æ³¨æˆ‘ä»¬', '', text)
    text = re.sub(r'è¡€æ¶²ç—…ä¸“å®¶\s+è¡€æ¶²ç—…ä¸“å®¶;\)', '', text) 

    # 5. ç§»é™¤æ®‹ç•™çš„å¹²æ‰°è¯ (å…œåº•)
    noise_patterns = [
        r'é¢„è§ˆæ—¶æ ‡ç­¾ä¸å¯ç‚¹',
        r'è½»è§¦é˜…è¯»åŸæ–‡',
        r'å¾®ä¿¡æ‰«ä¸€æ‰«',
        r'å…³æ³¨è¯¥å…¬ä¼—å·',
        r'javascript:void\(0\);',
        r'javascript:;',
        r'é˜…è¯»åŸæ–‡',
        r'æ”¶å½•äºåˆé›†',
        r'è§†é¢‘\s+å°ç¨‹åº\s+èµ',
        r'å–æ¶ˆ;\)\s+å…è®¸;\)' 
    ]
    for pattern in noise_patterns:
        text = re.sub(pattern, '', text)

    # 6. ç§»é™¤å¥‡æ€ªçš„æ ‡ç‚¹å †ç§¯è¡Œ (å¦‚: ï¼Œ ï¼Œ ï¼Œ)
    text = re.sub(r'^[ï¼Œã€‚ï¼šï¼›,\.\s]+$', '', text, flags=re.MULTILINE)

    # 7. åˆå¹¶å¤šä½™æ¢è¡Œ
    text = re.sub(r'\n\s*\n', '\n\n', text)
    
    return text.strip()

def extract_from_markdown(filepath):
    """
    è¯»å–æ–‡ä»¶ï¼Œæå–æ—¥æœŸå’Œæ ‡é¢˜ï¼Œå¹¶æ¸…æ´—
    """
    try:
        filename = os.path.basename(filepath)
        
        # ä»æ–‡ä»¶åæå–æ—¥æœŸ "20251204_æ ‡é¢˜.md"
        date_match = re.match(r'(\d{8})_(.*)\.md', filename)
        
        if date_match:
            publish_date = date_match.group(1)
            title = date_match.group(2)
        else:
            publish_date = "Unknown"
            title = filename.replace('.md', '').replace('.html', '')

        with open(filepath, 'r', encoding='utf-8') as f:
            raw_content = f.read()

        # è°ƒç”¨æ¸…æ´—
        cleaned_text = clean_markdown_content(raw_content)
        
        # å¦‚æœæ¸…æ´—å®Œåªå‰©ä¸‹æçŸ­çš„å†…å®¹ï¼Œè§†ä¸ºæ— æ•ˆ
        if len(cleaned_text) < 20: 
            return None

        return {
            "title": title,
            "publish_date": publish_date,
            "content": cleaned_text
        }
    except Exception as e:
        print(f"è¯»å–é”™è¯¯ {filepath}: {e}")
        return None

def smart_split_text(text, chunk_size=512, chunk_overlap=50):
    """
    æŒ‰æ®µè½ä¼˜å…ˆåˆ‡åˆ†
    """
    if not text:
        return []
    
    paragraphs = text.split('\n\n')
    chunks = []
    current_chunk = ""
    
    for para in paragraphs:
        para = para.strip()
        if not para: continue
            
        if len(current_chunk) + len(para) < chunk_size:
            current_chunk += "\n\n" + para
        else:
            if current_chunk:
                chunks.append(current_chunk.strip())
            
            if len(para) > chunk_size:
                # é•¿æ®µè½å¼ºåˆ‡
                for i in range(0, len(para), chunk_size - chunk_overlap):
                    chunks.append(para[i:i + chunk_size])
                current_chunk = ""
            else:
                current_chunk = para
                
    if current_chunk:
        chunks.append(current_chunk.strip())
        
    return chunks

# ================= ä¸»æ‰§è¡Œé€»è¾‘ =================
if __name__ == "__main__":
    all_data = []
    print(f"ğŸš€ å¼€å§‹å¤„ç†...")
    
    # å…¼å®¹ .md å’Œ .html
    files = [f for f in os.listdir(SOURCE_DIRECTORY) if f.endswith('.md') or f.endswith('.html')]
    
    for filename in files:
        filepath = os.path.join(SOURCE_DIRECTORY, filename)
        
        # 1. æå–ä¸æ¸…æ´—
        data = extract_from_markdown(filepath)
        
        if data and data['content']:
            # 2. åˆ‡åˆ†
            chunks = smart_split_text(data['content'], CHUNK_SIZE, CHUNK_OVERLAP)
            
            # 3. æ„å»ºæ•°æ®å— (åŠ å…¥ä¸»å¾ªç¯è¿‡æ»¤)
            for i, chunk in enumerate(chunks):
                
                # --- [æ–°å¢] ä¸»å¾ªç¯åƒåœ¾è¿‡æ»¤ ---
                # è¿‡æ»¤æ‰å¤ªçŸ­çš„å—ï¼ˆå°‘äº10ä¸ªå­—é€šå¸¸æ²¡æœ‰æ£€ç´¢ä»·å€¼ï¼‰
                if len(chunk.strip()) < 10:
                    continue
                # è¿‡æ»¤æ‰ä¾ç„¶æ®‹ç•™çš„å¯¼èˆªæ ç‰¹å¾
                if "å°ç¨‹åº" in chunk and "è§†é¢‘" in chunk:
                    continue
                # ---------------------------

                entry = {
                    "id": f"{filename}_{i}",
                    "title": data['title'],
                    "publish_date": data['publish_date'],
                    "abstract": chunk,
                    "source_file": filename,
                    "chunk_index": i,
                    "metadata": {
                        "source": "wechat_official",
                        "type": "medical_article"
                    }
                }
                all_data.append(entry)

    # 4. ä¿å­˜
    os.makedirs(os.path.dirname(OUTPUT_JSON_PATH), exist_ok=True)
    with open(OUTPUT_JSON_PATH, 'w', encoding='utf-8') as f:
        json.dump(all_data, f, ensure_ascii=False, indent=4)

    print(f"âœ… å¤„ç†å®Œæˆï¼")
    print(f"ğŸ“Š å…±ç”Ÿæˆ {len(all_data)} ä¸ªé«˜è´¨é‡æ•°æ®å—")
    print(f"ğŸ“ ç»“æœå·²ä¿å­˜è‡³: {OUTPUT_JSON_PATH}")
